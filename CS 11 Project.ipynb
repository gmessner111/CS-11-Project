{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.experimental import enable_hist_gradient_boosting \n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import ARDRegression\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import gplearn as gpl        \n",
    "from gplearn.genetic import SymbolicRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import and visualize the dataset\n",
    "data = pd.read_csv('bitstampUSD_1-min_data_2012-01-01_to_2019-08-12.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume_(BTC)</th>\n",
       "      <th>Volume_(Currency)</th>\n",
       "      <th>Weighted_Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1325317920</td>\n",
       "      <td>4.39</td>\n",
       "      <td>4.39</td>\n",
       "      <td>4.39</td>\n",
       "      <td>4.39</td>\n",
       "      <td>0.455581</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1325317980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1325318040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1325318100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1325318160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997692</th>\n",
       "      <td>1565567760</td>\n",
       "      <td>11555.57</td>\n",
       "      <td>11555.57</td>\n",
       "      <td>11540.37</td>\n",
       "      <td>11540.58</td>\n",
       "      <td>0.036868</td>\n",
       "      <td>425.909106</td>\n",
       "      <td>11552.336234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997693</th>\n",
       "      <td>1565567820</td>\n",
       "      <td>11553.49</td>\n",
       "      <td>11556.22</td>\n",
       "      <td>11553.49</td>\n",
       "      <td>11556.22</td>\n",
       "      <td>0.623462</td>\n",
       "      <td>7204.428272</td>\n",
       "      <td>11555.520505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997694</th>\n",
       "      <td>1565567880</td>\n",
       "      <td>11559.73</td>\n",
       "      <td>11561.22</td>\n",
       "      <td>11546.77</td>\n",
       "      <td>11561.22</td>\n",
       "      <td>0.159070</td>\n",
       "      <td>1838.731403</td>\n",
       "      <td>11559.252199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997695</th>\n",
       "      <td>1565567940</td>\n",
       "      <td>11559.73</td>\n",
       "      <td>11589.73</td>\n",
       "      <td>11528.73</td>\n",
       "      <td>11528.73</td>\n",
       "      <td>16.198210</td>\n",
       "      <td>187504.635170</td>\n",
       "      <td>11575.638889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997696</th>\n",
       "      <td>1565568000</td>\n",
       "      <td>11527.44</td>\n",
       "      <td>11551.57</td>\n",
       "      <td>11520.00</td>\n",
       "      <td>11520.00</td>\n",
       "      <td>23.805939</td>\n",
       "      <td>274731.256920</td>\n",
       "      <td>11540.450291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3997697 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Timestamp      Open      High       Low     Close  Volume_(BTC)  \\\n",
       "0        1325317920      4.39      4.39      4.39      4.39      0.455581   \n",
       "1        1325317980       NaN       NaN       NaN       NaN           NaN   \n",
       "2        1325318040       NaN       NaN       NaN       NaN           NaN   \n",
       "3        1325318100       NaN       NaN       NaN       NaN           NaN   \n",
       "4        1325318160       NaN       NaN       NaN       NaN           NaN   \n",
       "...             ...       ...       ...       ...       ...           ...   \n",
       "3997692  1565567760  11555.57  11555.57  11540.37  11540.58      0.036868   \n",
       "3997693  1565567820  11553.49  11556.22  11553.49  11556.22      0.623462   \n",
       "3997694  1565567880  11559.73  11561.22  11546.77  11561.22      0.159070   \n",
       "3997695  1565567940  11559.73  11589.73  11528.73  11528.73     16.198210   \n",
       "3997696  1565568000  11527.44  11551.57  11520.00  11520.00     23.805939   \n",
       "\n",
       "         Volume_(Currency)  Weighted_Price  \n",
       "0                 2.000000        4.390000  \n",
       "1                      NaN             NaN  \n",
       "2                      NaN             NaN  \n",
       "3                      NaN             NaN  \n",
       "4                      NaN             NaN  \n",
       "...                    ...             ...  \n",
       "3997692         425.909106    11552.336234  \n",
       "3997693        7204.428272    11555.520505  \n",
       "3997694        1838.731403    11559.252199  \n",
       "3997695      187504.635170    11575.638889  \n",
       "3997696      274731.256920    11540.450291  \n",
       "\n",
       "[3997697 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume_(BTC)</th>\n",
       "      <th>Volume_(Currency)</th>\n",
       "      <th>Weighted_Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.727978</td>\n",
       "      <td>0.727783</td>\n",
       "      <td>0.728220</td>\n",
       "      <td>0.727981</td>\n",
       "      <td>-0.086029</td>\n",
       "      <td>0.219983</td>\n",
       "      <td>0.727997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Open</th>\n",
       "      <td>0.727978</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>-0.039713</td>\n",
       "      <td>0.361751</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>High</th>\n",
       "      <td>0.727783</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>-0.039494</td>\n",
       "      <td>0.362273</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Low</th>\n",
       "      <td>0.728220</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>-0.039980</td>\n",
       "      <td>0.361131</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Close</th>\n",
       "      <td>0.727981</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.039745</td>\n",
       "      <td>0.361676</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Volume_(BTC)</th>\n",
       "      <td>-0.086029</td>\n",
       "      <td>-0.039713</td>\n",
       "      <td>-0.039494</td>\n",
       "      <td>-0.039980</td>\n",
       "      <td>-0.039745</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.395642</td>\n",
       "      <td>-0.039752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Volume_(Currency)</th>\n",
       "      <td>0.219983</td>\n",
       "      <td>0.361751</td>\n",
       "      <td>0.362273</td>\n",
       "      <td>0.361131</td>\n",
       "      <td>0.361676</td>\n",
       "      <td>0.395642</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.361667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted_Price</th>\n",
       "      <td>0.727997</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>-0.039752</td>\n",
       "      <td>0.361667</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Timestamp      Open      High       Low     Close  \\\n",
       "Timestamp           1.000000  0.727978  0.727783  0.728220  0.727981   \n",
       "Open                0.727978  1.000000  0.999999  0.999998  0.999998   \n",
       "High                0.727783  0.999999  1.000000  0.999997  0.999998   \n",
       "Low                 0.728220  0.999998  0.999997  1.000000  0.999998   \n",
       "Close               0.727981  0.999998  0.999998  0.999998  1.000000   \n",
       "Volume_(BTC)       -0.086029 -0.039713 -0.039494 -0.039980 -0.039745   \n",
       "Volume_(Currency)   0.219983  0.361751  0.362273  0.361131  0.361676   \n",
       "Weighted_Price      0.727997  0.999999  0.999999  0.999999  0.999999   \n",
       "\n",
       "                   Volume_(BTC)  Volume_(Currency)  Weighted_Price  \n",
       "Timestamp             -0.086029           0.219983        0.727997  \n",
       "Open                  -0.039713           0.361751        0.999999  \n",
       "High                  -0.039494           0.362273        0.999999  \n",
       "Low                   -0.039980           0.361131        0.999999  \n",
       "Close                 -0.039745           0.361676        0.999999  \n",
       "Volume_(BTC)           1.000000           0.395642       -0.039752  \n",
       "Volume_(Currency)      0.395642           1.000000        0.361667  \n",
       "Weighted_Price        -0.039752           0.361667        1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get an idea of what parameters are correlated most with open price (which is what we want to predict)\n",
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the raw time data from the dataframe\n",
    "times = data['Timestamp']\n",
    "numTimes = len(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Open price (USD)')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Get an idea of the BTC price as a function of time\n",
    "plt.plot(data['Timestamp'],data['Close'])\n",
    "plt.xlabel('Time (unix time)')\n",
    "plt.ylabel('Open price (USD)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the dataframe to a format we can actually use\n",
    "real_data = data.T.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1231878\n"
     ]
    }
   ],
   "source": [
    "#Remove rows that have NaN's in them \n",
    "trimmed_data = []\n",
    "nanCount = 0\n",
    "for i in range(numTimes):\n",
    "    if(np.isnan(real_data[1][i])):\n",
    "        nanCount = nanCount + 1\n",
    "    else:\n",
    "        trimmed_data.append(real_data.T[i])\n",
    "\n",
    "trimmed_data = np.array(trimmed_data)\n",
    "print(nanCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chop up entire dataset into training and test data, and get the lengths of both\n",
    "numUsedTimes = len(trimmed_data)\n",
    "train_data = trimmed_data[:int(9*numUsedTimes/10)]\n",
    "test_data = trimmed_data[int(9*numUsedTimes/10):]\n",
    "\n",
    "N_train = len(train_data)\n",
    "N_test = len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getXY(params, window_size):\n",
    "    '''Convert the raw training and testing data into matrices X, Y, and X_test. X is obtained by looking at the \n",
    "    data window_size time steps into the past, and collecting the training data at every index specified in params.\n",
    "    X_test is done very similarly with the test data, and Y is just the open price for each timestep.'''\n",
    "    \n",
    "    numParams = len(params)\n",
    "    X = np.zeros(shape=(N_train - window_size, numParams*window_size))\n",
    "    Y = np.zeros(N_train - window_size)\n",
    "    for i in range(N_train-window_size):\n",
    "        for j in range(numParams):\n",
    "            X[i][window_size*j:window_size*(j+1)] = train_data.T[params[j]][i:i+window_size]\n",
    "        Y[i] = train_data[i+window_size][1]\n",
    "    \n",
    "    X_test = np.zeros(shape=(N_test - window_size, numParams*window_size))\n",
    "    for i in range(N_test-window_size):\n",
    "        for j in range(numParams):\n",
    "            X_test[i][window_size*j:window_size*(j+1)] = test_data.T[params[j]][i:i+window_size]\n",
    "    \n",
    "    return X, Y, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredictions(X, Y, X_test, fitted_model, window_size,modelType='sklearn'):\n",
    "    '''Use the fitted_model to make predictions using X for the training set and X_test for the testing set. Note that\n",
    "    we have to get the predictions differently if we are using a neural net vs. an sklearn fit.'''\n",
    "    \n",
    "    y_pred_train = np.zeros(N_train)\n",
    "    y_pred_test = np.zeros(N_test)\n",
    "    for i in range(window_size):\n",
    "        y_pred_train[i] = train_data[i][1]\n",
    "        \n",
    "    if(modelType == 'sklearn'):\n",
    "        y_pred_train[window_size:] = fitted_model.predict(X)\n",
    "        y_pred_test[:window_size] = fitted_model.predict(X[len(X)-window_size:])\n",
    "        y_pred_test[window_size:] = fitted_model.predict(X_test)\n",
    "    \n",
    "    elif(modelType == 'pytorch'):\n",
    "        y_pred_train[window_size:] = fitted_model(torch.tensor(X,dtype=torch.float32)).detach().numpy().flatten()\n",
    "        y_pred_test[:window_size] = fitted_model(torch.tensor(X[len(X)-window_size:],dtype=torch.float32)).detach().numpy().flatten()\n",
    "        y_pred_test[window_size:] = fitted_model(torch.tensor(X_test,dtype=torch.float32)).detach().numpy().flatten()\n",
    "\n",
    "    return y_pred_train, y_pred_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_error(y_pred,y_real):\n",
    "    '''Standard squared error. Nothing to see here.'''\n",
    "    \n",
    "    return (y_pred-y_real)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getErrors(model,X,Y,X_test,window_size,verbose=False,modelType='sklearn',\n",
    "              n_epochs=100,y_r_train=[],y_r_test=[]):\n",
    "    '''Compute the squared error of the model by fitting to the data, making predictions, and then computing the\n",
    "    squared error loss.'''\n",
    "    \n",
    "    if(modelType =='sklearn'):\n",
    "        fit = model.fit(X,Y)\n",
    "    \n",
    "    elif(modelType == 'pytorch'):\n",
    "        fit = train_nn(model,n_epochs,1,X,Y)\n",
    "\n",
    "        \n",
    "    y_pred_train, y_pred_test = getPredictions(X, Y, X_test, fit, window_size, modelType=modelType)\n",
    "    \n",
    "    if(len(y_r_train)>0):\n",
    "        y_pred_train = y_pred_train -y_r_train\n",
    "        \n",
    "    if(len(y_r_test)>0):\n",
    "        y_pred_test = y_pred_test -y_r_test\n",
    "\n",
    "    e_in = sum(squared_error(y_pred_train,train_data.T[1]))/N_train\n",
    "    e_out = sum(squared_error(y_pred_test,test_data.T[1]))/N_test\n",
    "    \n",
    "    if(verbose==True):\n",
    "        print(test_data.T[1],y_pred_test)\n",
    "    \n",
    "    try:\n",
    "        if(verbose == True):\n",
    "            print(fit.coef_)\n",
    "    except:\n",
    "        print('Nonlinear fit')\n",
    "        \n",
    "    return  e_in, e_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.343478194217901 19.909692928317785\n"
     ]
    }
   ],
   "source": [
    "#Get a baseline for the prediction errors by just using the previous open price as a prediction for the next one\n",
    "y_pred_train_0 = np.zeros(N_train)\n",
    "y_pred_train_0[0] = train_data[0][4]\n",
    "for i in range(1,N_train):\n",
    "    y_pred_train_0[i] = train_data[i-1][4]\n",
    "\n",
    "y_pred_test_0 = np.zeros(N_test)\n",
    "y_pred_test_0[0] = test_data[0][4]\n",
    "for i in range(1,N_test):\n",
    "    y_pred_test_0[i] = test_data[i-1][4]\n",
    "\n",
    "e_in_0 = sum(squared_error(y_pred_train_0,train_data.T[1]))/N_train\n",
    "e_out_0 = sum(squared_error(y_pred_test_0,test_data.T[1]))/N_test\n",
    "print(e_in_0,e_out_0) #baseline errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first model we'll try is linear regression\n",
    "linear_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a few different datasets, based on different parameter choices and window sizes. Note that I tried a few others\n",
    "#as well, but these were the ones that performed the best\n",
    "X, Y, X_test = getXY([0,1,2,3,4,5,6],1)\n",
    "X_2, Y_2, X_test_2 = getXY([1,2,3,4,5,6],1)\n",
    "X_3, Y_3, X_test_3 = getXY([0,1,2,3,4,5,6],2)\n",
    "X_4, Y_4, X_test_4 = getXY([0,1,2,3,4,5,6],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12.96044633532434, 18.274036288104583)\n",
      "(12.960908094451439, 18.27592744536153)\n",
      "(12.904788744982097, 18.317776628808474)\n",
      "(12.899067872485176, 18.315128544208754)\n"
     ]
    }
   ],
   "source": [
    "#Compute the error rates of each model. Note that all errors here and below are in the form (e_in, e_out)\n",
    "print(getErrors(linear_model,X,Y,X_test,1))\n",
    "print(getErrors(linear_model,X_2,Y_2,X_test_2,1))\n",
    "print(getErrors(linear_model,X_3,Y_3,X_test_3,2))\n",
    "print(getErrors(linear_model,X_4,Y_4,X_test_4,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12.96192590839802, 18.3161145192644)\n",
      "(12.962387794733054, 18.318142169560755)\n",
      "(12.906178017998142, 18.355703899600606)\n",
      "(12.900463023247672, 18.353711716551196)\n"
     ]
    }
   ],
   "source": [
    "#Next, try some fits with a ridge regression model. These were always slightly worse than the linear fits, no matter\n",
    "#how I varied alpha.\n",
    "ridge_model = Ridge(alpha=1000000)\n",
    "print(getErrors(ridge_model,X,Y,X_test,1))\n",
    "print(getErrors(ridge_model,X_2,Y_2,X_test_2,1))\n",
    "print(getErrors(ridge_model,X_3,Y_3,X_test_3,2))\n",
    "print(getErrors(ridge_model,X_4,Y_4,X_test_4,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63.14126391837603, 111.73308039907207)\n",
      "(63.148799341389186, 111.73888540246773)\n",
      "(122.01398267222756, 219.137255120359)\n",
      "(180.8877698351098, 328.5087326685095)\n"
     ]
    }
   ],
   "source": [
    "#Same thing with the lasso model. None of these fits were very good no matter how I varied alpha.\n",
    "lasso_model = Lasso(alpha = .01)\n",
    "print(getErrors(lasso_model,X,Y,X_test,1))\n",
    "print(getErrors(lasso_model,X_2,Y_2,X_test_2,1))\n",
    "print(getErrors(lasso_model,X_3,Y_3,X_test_3,2))\n",
    "print(getErrors(lasso_model,X_4,Y_4,X_test_4,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1409.380053527461, 114216.1761391779)\n",
      "(1715.946453380439, 6418.491984906092)\n",
      "(832.3254314237316, 48344.85711313347)\n",
      "(634.8631672785721, 37195.68167855047)\n"
     ]
    }
   ],
   "source": [
    "#I tried a few other types of models. This histogram gradient boosting regressor was interesting, although ultimately\n",
    "#wasn't very effective, no matter how I tuned the hyperparameters.\n",
    "hist_gradient_boost_model = HistGradientBoostingRegressor(learning_rate = 1,max_iter=1000)\n",
    "print(getErrors(hist_gradient_boost_model,X,Y,X_test,1))\n",
    "print(getErrors(hist_gradient_boost_model,X_2,Y_2,X_test_2,1))\n",
    "print(getErrors(hist_gradient_boost_model,X_3,Y_3,X_test_3,2))\n",
    "print(getErrors(hist_gradient_boost_model,X_4,Y_4,X_test_4,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tried out a random forest regressor as well. Also wasn't super effective, and took forever to run.\n",
    "random_forest_model = RandomForestRegressor()\n",
    "print(getErrors(random_forest_model,X,Y,X_test,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A couple of other fits I tried that ended up crashing my kernel or my entire computer...\n",
    "\n",
    "#Do not run!\n",
    "#gaussian_process_model = GaussianProcessRegressor()\n",
    "#print(getErrors(gaussian_process_model,X,Y,X_test,1))\n",
    "\n",
    "#ard_model = ARDRegression()\n",
    "#print(getErrors(ard_model,X,Y,X_test,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(model,N_epochs,r,X,Y,eta=1e-1):\n",
    "    '''\n",
    "    Train the nn model for N_epochs using a ratio r of the training set.\n",
    "    The rest will be used as validation data.\n",
    "    '''\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=eta)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    n_max = int(r*len(X))\n",
    "    for epoch in range(N_epochs):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        Y_pred = model(torch.tensor(X[:n_max],dtype=torch.float32))\n",
    "        # Compute and print loss\n",
    "        loss = loss_fn(Y_pred, torch.tensor(np.transpose(np.matrix(Y[:n_max])),dtype=torch.float32))\n",
    "\n",
    "        if(epoch%int(N_epochs/10) == 0):\n",
    "            print('epoch: ', epoch,' loss: ', loss.item())\n",
    "    \n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # perform a backward pass (backpropagation)\n",
    "        loss.backward()\n",
    "    \n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=6, out_features=20, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=20, out_features=10, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=5, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Define a neural network to test\n",
    "nn_model = nn.Sequential( \n",
    "    nn.Linear(6, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20,10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10,5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5,1)\n",
    ")\n",
    "print(nn_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  16637088.0\n",
      "epoch:  40  loss:  389530.03125\n",
      "epoch:  80  loss:  4304.99755859375\n",
      "epoch:  120  loss:  214.20401000976562\n",
      "epoch:  160  loss:  109.1042709350586\n",
      "epoch:  200  loss:  86.4802474975586\n",
      "epoch:  240  loss:  70.82490539550781\n",
      "epoch:  280  loss:  59.43095016479492\n",
      "epoch:  320  loss:  51.11249923706055\n",
      "epoch:  360  loss:  45.00065612792969\n",
      "(40.46977179006838, 83.3539087087517)\n"
     ]
    }
   ],
   "source": [
    "#Run the neural network. Note that I used the _2 data here (without the timesteps) since I was getting better fits\n",
    "#this way. So far, I wasn't getting anything great out of the neural net, although it has some potential and moving\n",
    "#forward I think I'll continue to tinker with it. I also might use it on top of the linear fit I already have to try\n",
    "#and extract additional improvements on the current predictions.\n",
    "print(getErrors(nn_model,X_2,Y_2,X_test_2,1,modelType = 'pytorch',n_epochs=400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, try learning the residuals of a linear model and the true predictions using a neural net\n",
    "linear_model_2 = LinearRegression()\n",
    "fit = linear_model_2.fit(X_2,Y_2)\n",
    "y_pred_train, y_pred_test = getPredictions(X_2, Y_2, X_test_2, fit, 1,modelType='sklearn')\n",
    "Y_r = y_pred_train[1:] -Y_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  12.960774421691895\n",
      "epoch:  10  loss:  12.963347434997559\n",
      "epoch:  20  loss:  12.961282730102539\n",
      "epoch:  30  loss:  12.960771560668945\n",
      "epoch:  40  loss:  12.960783958435059\n",
      "epoch:  50  loss:  12.960772514343262\n",
      "epoch:  60  loss:  12.960771560668945\n",
      "epoch:  70  loss:  12.960771560668945\n",
      "epoch:  80  loss:  12.960768699645996\n",
      "epoch:  90  loss:  12.960769653320312\n"
     ]
    }
   ],
   "source": [
    "fit_nn = train_nn(nn_model,100,1,X_2,Y_r,eta=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the predicted residuals and then compute the squared loss\n",
    "y_r_train, y_r_test = getPredictions(X_2, Y_r, X_test_2, fit_nn, 1,modelType='pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12.960454294142252, 18.274072173805777)\n"
     ]
    }
   ],
   "source": [
    "#This was a small improvement over the linear fit on some runs, although the neural net was having a hard time \n",
    "#learning different predictions at each data point for whatever reason (i.e., all predicted residuals were the same).\n",
    "#I went back and checked and it's not an issue with my code, it's just what the neural net is actually learning.\n",
    "print(getErrors(linear_model,X,Y,X_test,1,y_r_train=y_r_train, y_r_test=y_r_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0     3.95          4426.17        2          4322.81              N/A      2.45m\n",
      "   1     2.85          4185.07        1          7.95067              N/A      1.31m\n",
      "   2     1.57          2086.98        1          3.91708              N/A     53.87s\n",
      "   3     1.32          889.432        1          3.91708              N/A     49.19s\n",
      "   4     1.38      3.65139e+07        1          3.91708              N/A     46.71s\n",
      "   5     1.27          446.831        1          3.91708              N/A     43.97s\n",
      "   6     1.25          778.809        1          3.91708              N/A     41.03s\n",
      "   7     1.18          556.245        1          3.91708              N/A     38.47s\n",
      "   8     1.15          446.858        1          3.91708              N/A     34.48s\n",
      "   9     1.45          888.574        1          3.91708              N/A     35.07s\n",
      "  10     1.23           556.38        1          3.91708              N/A     35.74s\n",
      "  11     1.52           1000.7        1          3.91708              N/A     31.33s\n",
      "  12     1.32          1779.83        1          3.91708              N/A     28.14s\n",
      "  13     1.12            336.2        1          3.91708              N/A     24.27s\n",
      "  14     1.20          442.778        1          3.91708              N/A     22.62s\n",
      "  15     1.20          557.518        1          3.91708              N/A     19.78s\n",
      "  16     1.12          336.065        1          3.91708              N/A     17.27s\n",
      "  17     1.18          556.248        1          3.91708              N/A     16.00s\n",
      "  18     1.65           1105.9        1          3.91708              N/A     15.63s\n",
      "  19     1.40          1000.55        1          3.91708              N/A     11.97s\n",
      "  20     1.30      3.65138e+07        1          3.91708              N/A     10.37s\n",
      "  21     1.27          996.301        1          3.91708              N/A      7.76s\n",
      "  22     1.50          3038.07        1          3.91708              N/A      4.70s\n",
      "  23     1.35      3.65137e+07        1          3.91708              N/A      2.28s\n",
      "  24     1.32          778.778        1          3.91708              N/A      0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SymbolicRegressor(const_range=(-1.0, 1.0), feature_names=None,\n",
       "                  function_set=['sin', 'cos', 'sqrt', 'log'], generations=25,\n",
       "                  init_depth=(2, 6), init_method='half and half',\n",
       "                  low_memory=False, max_samples=1.0, metric='rmse', n_jobs=-1,\n",
       "                  p_crossover=0.65, p_hoist_mutation=0.05, p_point_mutation=0.1,\n",
       "                  p_point_replace=0.05, p_subtree_mutation=0.15,\n",
       "                  parsimony_coefficient=0.001, population_size=40,\n",
       "                  random_state=None, stopping_criteria=0.1, tournament_size=10,\n",
       "                  verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Next, I worked with a symbolic regression model. This also produced decent results, although still was slightly \n",
    "#worse than the linear fit.\n",
    "def _xexp( x ):\n",
    "    a = np.exp(x); \n",
    "    a[ np.abs(a) > 1e+9 ] = 1e+9\n",
    "    return a    \n",
    "\n",
    "xexp = gpl.functions.make_function( function = _xexp, name='xexp', arity=1 )\n",
    "\n",
    "function_set = ['add', 'sub', 'mul', 'div','sin','cos','sqrt','log']\n",
    "model_sr = SymbolicRegressor(population_size = 40, tournament_size=10,\n",
    "                          generations = 25, stopping_criteria=0.1,\n",
    "                          function_set = function_set, metric='rmse',\n",
    "                          p_crossover=0.65, p_subtree_mutation=0.15,\n",
    "                          p_hoist_mutation=0.05, p_point_mutation=0.1,\n",
    "                          verbose = 1, random_state = None, n_jobs = -1)\n",
    "\n",
    "model_sr.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.343478194217901 19.90968681801415\n"
     ]
    }
   ],
   "source": [
    "#e_out of 19.9 is slightly higher than our best predictions of ~18.2 ish\n",
    "sr_y_pred_train = model_sr.predict(X)\n",
    "sr_y_pred_test = model_sr.predict(X_test)\n",
    "e_in = sum(squared_error(sr_y_pred_train,train_data.T[1][1:]))/N_train\n",
    "e_out = sum(squared_error(sr_y_pred_test,test_data.T[1][1:]))/N_test\n",
    "print(e_in,e_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, try learning the residuals without the most recent close price, to see if this could generate better predictions.\n",
    "X_r, Y_same, X_r_test = getXY([0,1,2,3,5,6],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_r_2 = Y - y_pred_train_0[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    21.00      6.62821e+27        7          3.91708              N/A      5.80m\n",
      "   1    24.45      5.19961e+10       15          3.91706              N/A      3.63m\n",
      "   2     9.15      1.82782e+08       15          3.91706              N/A      2.01m\n",
      "   3     4.25          2696.36        7          3.91708              N/A      1.31m\n",
      "   4     3.15      1.69609e+11        5          3.91708              N/A      1.19m\n",
      "   5     2.85      3.49802e+12        5          3.91708              N/A      1.18m\n",
      "   6     3.30      1.46056e+08        5          3.91708              N/A     55.63s\n",
      "   7     3.10          336.739        5          3.91708              N/A      1.08m\n",
      "   8     3.50          4027.45        5          3.91708              N/A      1.08m\n",
      "   9     5.30      3.40144e+09        5          3.91708              N/A      1.10m\n",
      "  10     3.45      3.06555e+19        5          3.91708              N/A     49.72s\n",
      "  11     3.70      2.75584e+24        5          3.91708              N/A     43.64s\n",
      "  12     4.30      8.63825e+07        5          3.91708              N/A     45.25s\n",
      "  13     3.65      3.65139e+07        3          3.91708              N/A     41.22s\n",
      "  14     3.65      3.37151e+11        5          3.91708              N/A     35.44s\n",
      "  15     3.05      7.30266e+07        5          3.91708              N/A     28.79s\n",
      "  16     3.15       1.4855e+19        5          3.91708              N/A     25.78s\n",
      "  17     3.00      2.19522e+08        5          3.91708              N/A     21.08s\n",
      "  18     3.15      1.41685e+09        5          3.91708              N/A     19.96s\n",
      "  19     3.20      1.68417e+11        5          3.91708              N/A     15.74s\n",
      "  20     2.75      1.55643e+08        5          3.91708              N/A     11.68s\n",
      "  21     3.00      1.34049e+08        3          3.91708              N/A      9.47s\n",
      "  22     3.05      5.10524e+14        3          3.91708              N/A      5.24s\n",
      "  23     4.05      2.04887e+15        3          3.91708              N/A      3.06s\n",
      "  24     3.50      4.30095e+13        3          3.91708              N/A      0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SymbolicRegressor(const_range=(-1.0, 1.0), feature_names=None,\n",
       "                  function_set=['add', 'sub', 'mul', 'div'], generations=25,\n",
       "                  init_depth=(2, 6), init_method='half and half',\n",
       "                  low_memory=False, max_samples=1.0, metric='rmse', n_jobs=-1,\n",
       "                  p_crossover=0.65, p_hoist_mutation=0.05, p_point_mutation=0.1,\n",
       "                  p_point_replace=0.05, p_subtree_mutation=0.15,\n",
       "                  parsimony_coefficient=0.001, population_size=40,\n",
       "                  random_state=None, stopping_criteria=0.1, tournament_size=10,\n",
       "                  verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First, I tried fitting a symbolic regressor to the residuals. It wasn't able to generate much of an improvement.\n",
    "function_set = ['add', 'sub', 'mul', 'div']#,'sin','cos','sqrt','log']\n",
    "model_sr = SymbolicRegressor(population_size = 40, tournament_size=10,\n",
    "                          generations = 25, stopping_criteria=0.1,\n",
    "                          function_set = function_set, metric='rmse',\n",
    "                          p_crossover=0.65, p_subtree_mutation=0.15,\n",
    "                          p_hoist_mutation=0.05, p_point_mutation=0.1,\n",
    "                          verbose = 1, random_state = None, n_jobs = -1)\n",
    "\n",
    "model_sr.fit(X_r, Y_r_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_r_train_2 = model_sr.predict(X_r)\n",
    "X_r_test_2 = np.zeros(shape=(len(X_r_test)+1,len(X_r_test[0])))\n",
    "X_r_test_2[0] = X_r[len(X_r)-1]\n",
    "X_r_test_2[1:] = X_r_test\n",
    "y_r_test_2 = model_sr.predict(X_r_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.909692928317785\n"
     ]
    }
   ],
   "source": [
    "e_out_0_adj = sum(squared_error(y_pred_test_0-y_r_test_2,test_data.T[1]))/N_test\n",
    "print(e_out_0_adj)\n",
    "#Basically the same e_out as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=6, out_features=20, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=20, out_features=10, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=5, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Next, I tried fitting a neural net to the residuals, excluding the previous close price. This gave an improvement over \n",
    "#that price, but still could be better. Also, this neural net is still predicting the same value for each timestep, \n",
    "#even though it is set up correctly. \n",
    "layer1 = nn.Linear(6, 20)\n",
    "layer2 = nn.Linear(20,10)\n",
    "layer3 = nn.Linear(10,5)\n",
    "layer4 = nn.Linear(5,1)\n",
    "\n",
    "nn_model_2 = nn.Sequential( \n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    layer2,\n",
    "    nn.ReLU(),\n",
    "    layer3,\n",
    "    nn.ReLU(),\n",
    "    layer4\n",
    ")\n",
    "print(nn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  21319624163328.0\n",
      "epoch:  10  loss:  15.345826148986816\n",
      "epoch:  20  loss:  15.369014739990234\n",
      "epoch:  30  loss:  15.385180473327637\n",
      "epoch:  40  loss:  15.392054557800293\n",
      "epoch:  50  loss:  15.395057678222656\n",
      "epoch:  60  loss:  15.396120071411133\n",
      "epoch:  70  loss:  15.39639663696289\n",
      "epoch:  80  loss:  15.396484375\n",
      "epoch:  90  loss:  15.396592140197754\n"
     ]
    }
   ],
   "source": [
    "fit_nn = train_nn(nn_model_2,100,1,X_r,Y_r_2,eta=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_r_train_3, y_r_test_3 = getPredictions(X_r, Y_r_2, X_r_test, fit_nn, 1,modelType='pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13.006343617903292, 18.33634210223066)\n"
     ]
    }
   ],
   "source": [
    "#Performs decently when used to adjust the linear model. Next, I might try using the linear model residuals to fit.\n",
    "print(getErrors(linear_model,X,Y,X_test,1,y_r_train=y_r_train_3, y_r_test=y_r_test_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
